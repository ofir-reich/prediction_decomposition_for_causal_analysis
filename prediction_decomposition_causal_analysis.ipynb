{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install linearmodels"
      ],
      "metadata": {
        "id": "OfHZTBCXaWbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO_RIqDLZhNI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import tqdm\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import statsmodels.formula.api as smf\n",
        "from linearmodels.panel import PanelOLS\n",
        "\n",
        "import warnings\n",
        "# Suppress specific UserWarning from scikit-learn\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning, module='sklearn')\n",
        "\n",
        "\n",
        "##################\n",
        "# Configurations #\n",
        "##################\n",
        "# Pandas\n",
        "pd.set_option(\"display.max_rows\", 50, \"display.max_columns\", None, \"display.width\", 200)\n",
        "pd.options.display.precision = 3  # 3 significant digits\n",
        "\n",
        "# Plotly\n",
        "plotly.io.renderers.default = \"colab\"\n",
        "plotly.io.templates.default = \"plotly_white\"\n",
        "pd.options.plotting.backend = \"plotly\"\n",
        "\n",
        "EPSILON = 1E-6\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in double_scalars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUGIcf89ZhNL"
      },
      "outputs": [],
      "source": [
        "def add_y_equal_x_line(fig, x_min=None, x_max=None):\n",
        "    \"\"\"Make a plotly scatterplot figure scale ratio 1:1 and add dashed y=x line.\"\"\"\n",
        "    fig.update_yaxes(\n",
        "        scaleanchor = \"x\",\n",
        "        scaleratio = 1,\n",
        "        )\n",
        "\n",
        "    if x_min is None:\n",
        "        x_min, x_max = fig.layout.xaxis.range[0], fig.layout.xaxis.range[1]\n",
        "    min_max = (x_min, x_max)\n",
        "\n",
        "    fig.update_layout(width=500)\n",
        "    fig.add_trace(go.Scatter(x=min_max, y=min_max,\n",
        "                             mode='lines',\n",
        "                             line=dict(color='gray', dash='dash', width=2),\n",
        "                             opacity=0.5,\n",
        "                             name='y=x'))\n",
        "    return fig\n",
        "\n",
        "def is_fe_var(variable_name):\n",
        "    \"\"\"Whether the variable name indicates a Fixed Effect, C(<something>).\"\"\"\n",
        "    return bool(re.match(r'C\\(.*\\)', variable_name))\n",
        "\n",
        "\n",
        "def has_fe(model):\n",
        "    \"\"\"Whether the model has Fixed Effects.\"\"\"\n",
        "    exog_names = model.exog_names\n",
        "    if any([is_fe_var(exog_name) for exog_name in exog_names]):\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        return 'No'\n",
        "\n",
        "\n",
        "def reg_summary(fitted_model, exclude_fe=True):\n",
        "    \"\"\"Regression summary as a DataFrame, not the statsmodels returned object.\n",
        "\n",
        "    :param fitted_model: statsmodels regression model, with .fit()\n",
        "    :param exclude_fe: Whether to exclude Fixed Effects variables in the returned coefficients table.\n",
        "\n",
        "    :return: DataFrame with coefficients, standard errors, p-values, CIs etc.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        sm_summary = fitted_model.summary()\n",
        "    except TypeError:\n",
        "        sm_summary = fitted_model.summary\n",
        "    summary_df = pd.DataFrame(sm_summary.tables[1][1:], columns=sm_summary.tables[1][0])\n",
        "    summary_df.columns = summary_df.columns.astype(str)\n",
        "    summary_df.rename(columns={'': 'variable'}, inplace=True)\n",
        "    # Change data types.\n",
        "    summary_df['variable'] = summary_df['variable'].astype(str)\n",
        "    for col_name in summary_df.columns[1:]:\n",
        "        summary_df[col_name] = summary_df[col_name].astype(str).astype(float)\n",
        "    if exclude_fe:\n",
        "        summary_df = summary_df[~summary_df['variable'].apply(is_fe_var)]\n",
        "\n",
        "    return summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbAWFU10ZhNL"
      },
      "source": [
        "# Simulation and prediction funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a10-ZuPXZhNN"
      },
      "outputs": [],
      "source": [
        "def simulate_outcomes(n_experiment_persons=13000, mean_outcome=3200, std_dev_outcome=1400):\n",
        "    \"\"\"Simulate log-normal outcomes for n_experiment_persons with mean and std_dev_outcome.\"\"\"\n",
        "    # actual_outcomes = np.random.normal(mean_outcome, std_dev_outcome, n_experiment_persons)\n",
        "\n",
        "    # mean = exp(mu + sigma^2/2)\n",
        "    # var = [exp(sigma^2) - 1] * exp(2*mu + sigma^2)\n",
        "    # -> mu = log(mean^2 / sqrt(var + mean^2))\n",
        "    # -> sigma^2 = log(var / mean^2 + 1)\n",
        "    mu = np.log(mean_outcome**2 / np.sqrt(std_dev_outcome**2 + mean_outcome**2))\n",
        "    sigma = np.sqrt(np.log(std_dev_outcome**2 / mean_outcome**2 + 1))\n",
        "    actual_outcomes = np.random.lognormal(mu, sigma, n_experiment_persons)\n",
        "\n",
        "    return actual_outcomes\n",
        "\n",
        "# actual_outcomes = simulate_outcomes()\n",
        "# px.histogram(actual_outcomes, nbins=100, title=\"Histogram of Actual outcomes\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLA71jePZhNN"
      },
      "outputs": [],
      "source": [
        "def simulate_experiment_data(n_experiment_persons=13000, n_collected_outcomes=1200, mean_outcome=3200, std_dev_outcome=1400,\n",
        "                             treatment_effect=200, noise_sd=3000):\n",
        "    data_by_person = pd.DataFrame({'actual_outcome': simulate_outcomes(n_experiment_persons, mean_outcome, std_dev_outcome)})\n",
        "    data_by_person['is_treatment'] = np.random.rand(n_experiment_persons) < 0.5\n",
        "    data_by_person['actual_outcome'] += data_by_person['is_treatment'] * treatment_effect\n",
        "    # Gives about SD=600 for the predictions, where for the actual outcomes, the SD is 1400.\n",
        "    data_by_person['outcome_noisy_predictor'] = data_by_person['actual_outcome'] + np.random.normal(0, noise_sd, n_experiment_persons)\n",
        "\n",
        "    data_by_person['is_test'] = np.random.rand(n_experiment_persons) < (n_experiment_persons - n_collected_outcomes)/n_experiment_persons\n",
        "    return data_by_person\n",
        "\n",
        "def predict_outcomes(data_by_person, verbose=False):\n",
        "    \"\"\"Predict actual_outcome from outcome_noisy_predictor using sklearn LinearRegression, and two other methods.\"\"\"\n",
        "    # Reshape data for scikit-learn\n",
        "    # Features matrix X needs to be 2D (n_samples, n_features)\n",
        "    X = data_by_person[['outcome_noisy_predictor']].values\n",
        "    y = data_by_person['actual_outcome']  # Target variable\n",
        "\n",
        "    X_train = data_by_person.query('not is_test')[['outcome_noisy_predictor']].values\n",
        "    X_test = data_by_person.query('is_test')[['outcome_noisy_predictor']].values\n",
        "    y_train = data_by_person.query('not is_test')['actual_outcome'].values\n",
        "    y_test = data_by_person.query('is_test')['actual_outcome'].values\n",
        "\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(n_experiment_persons - n_collected_outcomes)/n_experiment_persons, random_state=42)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the testing set\n",
        "    y_pred = model.predict(X_test)\n",
        "    data_by_person['predicted_outcome'] = model.predict(data_by_person[['outcome_noisy_predictor']])\n",
        "\n",
        "    data_by_person['mean_outcome'] = mean_outcome = data_by_person['actual_outcome'].mean()\n",
        "    data_by_person['compressed_outcome'] = mean_outcome + (data_by_person['actual_outcome'] - mean_outcome) * 0.3\n",
        "\n",
        "    # Evaluate the model\n",
        "    rmse_pred = np.sqrt(mean_squared_error(data_by_person['actual_outcome'], data_by_person['predicted_outcome']))\n",
        "\n",
        "    # Evaluate benchmark constant model\n",
        "    rmse_const = np.sqrt(mean_squared_error(data_by_person['actual_outcome'], data_by_person['mean_outcome']))\n",
        "\n",
        "    # Evaluate compressed y\n",
        "    rmse_compressed = np.sqrt(mean_squared_error(data_by_person['actual_outcome'], data_by_person['compressed_outcome']))\n",
        "    if verbose:\n",
        "        print(f\"rmse_pred: {rmse_pred}\")\n",
        "        print(f\"rmse_const: {rmse_const}\")\n",
        "        print(f\"rmse_compressed: {rmse_compressed}\")\n",
        "\n",
        "    # px.histogram(pd.concat([pd.DataFrame({'outcome': y_pred, 'source': 'predicted'}), pd.DataFrame({'outcome': y_test, 'source': 'actual'})]),\n",
        "    #             x='outcome', color='source', histnorm='probability', barmode='overlay', opacity=0.5, title=\"Histogram of Predicted and Actual outcomes\").show()\n",
        "\n",
        "    return data_by_person"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtqBS_0eZhNN"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9v0MVaxZhNN"
      },
      "outputs": [],
      "source": [
        "data_by_person = simulate_experiment_data(treatment_effect=200, noise_sd=3000)\n",
        "data_by_person = predict_outcomes(data_by_person)\n",
        "\n",
        "# px.histogram(pd.concat([pd.DataFrame({'outcome': data_by_person['actual_outcome'], 'source': 'actual'}), pd.DataFrame({'outcome': data_by_person['predicted_outcome'], 'source': 'predicted'})]),\n",
        "#              x='outcome', color='source', histnorm='probability', barmode='overlay', opacity=0.5, title=\"Histogram of Predicted and Actual outcomes\", width=1000).show()\n",
        "\n",
        "# print(data_by_person[['actual_outcome', 'predicted_outcome']].describe())\n",
        "\n",
        "print(smf.ols('predicted_outcome ~ is_treatment', data=data_by_person).fit().summary())\n",
        "print('\\n'+'-'*120+'\\n')\n",
        "\n",
        "data_by_person['predicted_outcome'] = data_by_person['outcome_noisy_predictor']\n",
        "\n",
        "print(smf.ols('predicted_outcome ~ is_treatment', data=data_by_person).fit().summary())\n",
        "print('\\n'+'-'*120+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyYKsXvBZhNO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# fig = data_by_person.plot.scatter(x='outcome_noisy_predictor', y='actual_outcome', title=\"Actual outcomes vs. Noisy Predictor\", opacity=0.03)\n",
        "# add_y_equal_x_line(fig, *data_by_person['actual_outcome'].agg(['min', 'max']).tolist()).show()\n",
        "\n",
        "# print(smf.ols('actual_outcome ~ is_treatment', data=data_by_person).fit().summary())\n",
        "# print('\\n'+'-'*120+'\\n')\n",
        "# print(smf.ols('predicted_outcome ~ is_treatment', data=data_by_person).fit().summary())\n",
        "# print('\\n'+'-'*120+'\\n')\n",
        "# print(smf.ols('compressed_outcome ~ is_treatment', data=data_by_person).fit().summary())\n",
        "\n",
        "TREATMENT_EFFECT = 200\n",
        "\n",
        "res_list = []\n",
        "for i in range(100):\n",
        "    noise_sd = np.clip(np.random.normal(3000, 1000), 500, np.inf)\n",
        "    data_by_person = simulate_experiment_data(treatment_effect=TREATMENT_EFFECT, noise_sd=noise_sd)\n",
        "    data_by_person = predict_outcomes(data_by_person)\n",
        "    # data_by_person['predicted_outcome'] = data_by_person['outcome_noisy_predictor']\n",
        "\n",
        "    r2 = r2_score(data_by_person['actual_outcome'], data_by_person['predicted_outcome'])\n",
        "    rmse = mean_squared_error(data_by_person['actual_outcome'], data_by_person['predicted_outcome'])**0.5\n",
        "    pred_sd = np.std(data_by_person['predicted_outcome'])\n",
        "    actual_sd = np.std(data_by_person['actual_outcome'])\n",
        "\n",
        "    fitted_model = smf.ols('predicted_outcome ~ is_treatment', data=data_by_person).fit()\n",
        "    coef, std_err, p_value = reg_summary(fitted_model)[['coef', 'std err', 'P>|t|']].iloc[1].tolist()\n",
        "    coef, std_err, p_value\n",
        "\n",
        "    res_list.append({'coef': coef, 'std_err': std_err, 'p_value': p_value, 'treatment_effect': 200, 'noise_sd': noise_sd, 'pred_sd': pred_sd, 'actual_sd': actual_sd, 'r2': r2, 'rmse': rmse})\n",
        "\n",
        "res_df = pd.DataFrame(res_list)\n",
        "res_df['pred_compression'] = res_df['pred_sd'] / res_df['actual_sd']\n",
        "res_df['coef_compression'] = res_df['coef'] / TREATMENT_EFFECT\n",
        "\n",
        "res_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mudh0czBZhNP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "res_df.plot.scatter(x='r2', y='pred_sd')\n",
        "res_df.plot.scatter(x='r2', y='noise_sd')\n",
        "res_df.plot.scatter(x='r2', y='pred_compression')\n",
        "res_df.plot.scatter(x='r2', y=['coef', 'std_err'])\n",
        "res_df.plot.scatter(x='r2', y='p_value')\n",
        "\n",
        "fig = res_df.plot.scatter(x='pred_compression', y='coef_compression', title=\"Compression of Predicted SD vs. Coefficient\")\n",
        "add_y_equal_x_line(fig, 0, 1).show()\n",
        "\n",
        "fig = res_df.plot.scatter(x='r2', y='coef_compression', title=\"R2 vs. Coefficient Compression\")\n",
        "add_y_equal_x_line(fig, 0, 1).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzhPCYPzZhNQ"
      },
      "source": [
        "# Prediction decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "8vrMgNQ2277H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UEbAEFzZhNQ"
      },
      "outputs": [],
      "source": [
        "def generate_data_by_person_time(n_experiment_persons=13000, mean_mu=3200, std_dev_mu=1400,\n",
        "    treatment_effect=200, person_time_error_sd=400, n_time_periods=2) -> pd.DataFrame:\n",
        "    \"\"\"Simulates data by person and time period, with person fixed-effects, treatment effect, and noise.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with columns:\n",
        "        'person_id', 'time' (0 to n_time_periods-1), # DataFrame is unique by 'person_id' and 'time'.\n",
        "        'intercept', 'mu' (person fixed-effects), 'is_treatment' (random bernoulli(0.5) per person),\n",
        "        'treatment_effect' (0 for untreated, treatment_effect for treated),\n",
        "        'error_epsilon' (noise by person-time),\n",
        "        'actual_outcome' (=intercept + mu + treatment_effect + error_epsilon)\n",
        "    \"\"\"\n",
        "    data_by_person = pd.DataFrame({'mu': simulate_outcomes(n_experiment_persons, mean_mu, std_dev_mu), 'person_id': np.arange(n_experiment_persons)})\n",
        "    # Make mu have mean 0. Export its mean to 'intercept'.\n",
        "    intercept = data_by_person['mu'].mean()\n",
        "    data_by_person['intercept'] = intercept\n",
        "    data_by_person['mu'] -= intercept\n",
        "\n",
        "    data_by_person['is_treatment'] = np.random.rand(n_experiment_persons) < 0.5\n",
        "    data_by_person_time = data_by_person.merge(pd.DataFrame({'time': np.arange(n_time_periods)}), how='cross')\n",
        "    gamma = treatment_effect\n",
        "    data_by_person_time['treatment_effect'] = data_by_person_time['is_treatment'] * gamma\n",
        "    data_by_person_time['error_epsilon'] = np.random.normal(0, person_time_error_sd, len(data_by_person_time))\n",
        "    data_by_person_time['actual_outcome'] = data_by_person_time['intercept'] + data_by_person_time['mu'] + data_by_person_time['treatment_effect'] + data_by_person_time['error_epsilon']\n",
        "\n",
        "    return data_by_person_time\n",
        "\n",
        "def add_imperfect_prediction(data_by_person_time, eta_mu, eta_treatment, eta_epsilon, pred_error_nu_sd):\n",
        "    \"\"\"Adds column 'predicted_outcome' do the DataFrame, with imperfect prediction with the given eta's.\n",
        "\n",
        "    predicted_outcome = intercept + eta_mu * mu + eta_treatment * treatment_effect + eta_epsilon * error_epsilon + pred_error_nu\n",
        "    \"\"\"\n",
        "    data_by_person_time['pred_error_nu'] = np.random.normal(0, pred_error_nu_sd, len(data_by_person_time))\n",
        "    data_by_person_time['predicted_outcome'] = (\n",
        "        data_by_person_time['intercept'] +\n",
        "        eta_mu * data_by_person_time['mu'] +\n",
        "        eta_treatment * data_by_person_time['treatment_effect'] +\n",
        "        eta_epsilon * data_by_person_time['error_epsilon'] +\n",
        "        data_by_person_time['pred_error_nu']\n",
        "        )\n",
        "\n",
        "    return data_by_person_time\n",
        "\n",
        "def get_diffs_by_person(data_by_person_time):\n",
        "    \"\"\"Gets actual(time1) - actual(time0) and the same for predicted. By person. Assumes only two time periods.\"\"\"\n",
        "    data_by_person_time = data_by_person_time.copy()\n",
        "    assert data_by_person_time['time'].nunique() == 2\n",
        "    # grouped_by_person = data_by_person_time.sort_values(['person_id', 'time']).groupby('person_id')\n",
        "    # grouped_by_person['actual_outcome'].nth(1)\n",
        "\n",
        "    data_by_person_time.set_index(['person_id', 'time'], inplace=True)\n",
        "\n",
        "    # Unstack the 'time' level to pivot the data\n",
        "    data_by_person_time_unstacked = data_by_person_time[['actual_outcome', 'predicted_outcome']].unstack(level='time')\n",
        "\n",
        "    # Calculate the difference between the times for 'outcome' and 'pred'\n",
        "    diff_outcome_by_person = data_by_person_time_unstacked['actual_outcome'].diff(axis=1).dropna(axis=1)  # Drops the first column of NaNs after diff\n",
        "    diff_outcome_by_person.columns = ['actual_outcome_diff']\n",
        "\n",
        "    diff_pred_outcome_by_person = data_by_person_time_unstacked['predicted_outcome'].diff(axis=1).dropna(axis=1)\n",
        "    diff_pred_outcome_by_person.columns = ['predicted_outcome_diff']\n",
        "\n",
        "    # Combine the differences into a single DataFrame\n",
        "    pred_actual_by_person = pd.concat([diff_outcome_by_person, diff_pred_outcome_by_person], axis=1).reset_index()\n",
        "    return pred_actual_by_person\n",
        "\n",
        "def test_diff_prediction_diff_actual_regression(data_by_person_time) -> (float, float):\n",
        "    \"\"\"Regresses diff of predictions to diff of actuals. Returns coefficient, std error, and R^2.\"\"\"\n",
        "    pred_actual_by_person = get_diffs_by_person(data_by_person_time)\n",
        "\n",
        "    fitted_model = smf.ols('predicted_outcome_diff ~ actual_outcome_diff - 1', data=pred_actual_by_person).fit()\n",
        "    summary_df = reg_summary(fitted_model)\n",
        "    coef, std_err = summary_df.query('variable == \"actual_outcome_diff\"')[['coef', 'std err']].iloc[0].tolist()\n",
        "    return (coef,\n",
        "            std_err,\n",
        "            fitted_model.rsquared)\n",
        "\n",
        "def test_within_person_prediction(data_by_person_time) -> float:\n",
        "    \"\"\"Regress predicted outcome on actual outcome with person fixed-effects.\n",
        "\n",
        "    predictedOutcome_i,t ~ acutalOutcome_i,t + personFixedEffects_i\n",
        "    This is the generalization of test_diff_prediction_diff_actual_regression.\n",
        "\n",
        "    Returns coefficient, standard error, and R^2.\n",
        "    \"\"\"\n",
        "    data_by_person_time = data_by_person_time.copy()\n",
        "    # Convert data to panel format\n",
        "    panel_data = data_by_person_time.set_index(['person_id', 'time'])\n",
        "\n",
        "    # Define the model\n",
        "    model = PanelOLS.from_formula('predicted_outcome ~ actual_outcome + EntityEffects', panel_data)\n",
        "\n",
        "    # Fit the model\n",
        "    try:\n",
        "        fitted_model = model.fit()\n",
        "    except ZeroDivisionError:\n",
        "        if data_by_person_time.groupby('person_id')['predicted_outcome'].var().max() == 0:\n",
        "            # Predicted Outcome is constant within person, regression gives an error\n",
        "            # so return NaN for coef and standard error, r_squared=1.\n",
        "            r_squared = 1\n",
        "            return (np.nan, np.nan, r_squared)\n",
        "        elif (data_by_person_time['actual_outcome'] == data_by_person_time['predicted_outcome']).all():\n",
        "            # actual_outcome == predicted_outcome, this causes an error for PanelOLS.\n",
        "            return (1, 0, 1)\n",
        "        else:\n",
        "            # Something unexpected.\n",
        "            raise\n",
        "\n",
        "    summary_df = reg_summary(fitted_model)\n",
        "    coef, std_err = summary_df.query('variable == \"actual_outcome\"')[['Parameter', 'Std. Err.']].iloc[0].tolist()\n",
        "    return (coef,\n",
        "            std_err,\n",
        "            fitted_model.rsquared)\n",
        "\n",
        "def test_person_fixed_effects_explained_variance(data_by_person_time) -> float:\n",
        "    \"\"\"How much of the variance in actual outcome is explained by person fixed effects.\n",
        "\n",
        "    This is even more than (st_dev_mu**2 + st_dev_treatment**2) / (st_dev_mu**2 + st_dev_treatment**2 + st_dev_epsilon**2), because it's the empirical mean of each person.\n",
        "    \"\"\"\n",
        "    data_by_person_time = data_by_person_time.copy()\n",
        "    # Calculating explicitly, because the regression takes too long.\n",
        "    mean_outcome_by_person = data_by_person_time.groupby('person_id')['actual_outcome'].mean().rename('person_mean_outcome').reset_index()\n",
        "\n",
        "    data_by_person_time = data_by_person_time.merge(mean_outcome_by_person, on='person_id')\n",
        "    return r2_score(data_by_person_time['actual_outcome'], data_by_person_time['person_mean_outcome'])\n",
        "\n",
        "def test_treatment_effect_on_predictions(data_by_person_time) -> tuple[float]:\n",
        "    \"\"\"Treatment effect regression results on predicted outcome.\"\"\"\n",
        "    fitted_model = smf.ols('predicted_outcome ~ is_treatment', data=data_by_person_time.astype({'is_treatment': int})).fit()\n",
        "    summary_df = reg_summary(fitted_model)\n",
        "    coef, std_err, t_stat, p_value = summary_df.query('variable == \"is_treatment\"')[['coef', 'std err', 't', 'P>|t|']].iloc[0]\n",
        "    return coef, std_err, t_stat\n",
        "\n",
        "def calc_prediction_r2(data_by_person_time) -> float:\n",
        "    return r2_score(data_by_person_time['actual_outcome'], data_by_person_time['predicted_outcome'])\n",
        "\n",
        "def calc_prediction_stats(data_by_person_time) -> pd.DataFrame:\n",
        "    \"\"\"Returns DataFrame with various summary stats of the prediction performance/stats in various tests.\"\"\"\n",
        "    result_dict = {}\n",
        "\n",
        "    result_dict['prediction_r2'] = calc_prediction_r2(data_by_person_time)\n",
        "    # result_dict['diff_pred_coef'], result_dict['diff_pred_std_err'], result_dict['diff_pred_r2'] = test_diff_prediction_diff_actual_regression(data_by_person_time)\n",
        "    result_dict['diff_pred_coef'], result_dict['diff_pred_std_err'], result_dict['diff_pred_r2'] = test_within_person_prediction(data_by_person_time)\n",
        "\n",
        "    result_dict['actual_outcome_sd'] = data_by_person_time['actual_outcome'].std()\n",
        "    result_dict['predicted_outcome_sd'] = data_by_person_time['predicted_outcome'].std()\n",
        "    result_dict['compression'] = result_dict['predicted_outcome_sd'] / result_dict['actual_outcome_sd']\n",
        "\n",
        "    # This is even more than 1 - (st_dev_epsilon**2) / (st_dev_mu**2 + st_dev_treatment**2 + st_dev_epsilon**2), because it's the empirical mean of each person.\n",
        "    result_dict['var_explained_person'] = test_person_fixed_effects_explained_variance(data_by_person_time)\n",
        "    # (1400**2 + 100**2) / (1400**2 + 100**2 + 400**2)\n",
        "\n",
        "    result_dict['treatment_coef'], result_dict['treatment_se'], result_dict['treatment_t'] = test_treatment_effect_on_predictions(data_by_person_time)\n",
        "\n",
        "    return pd.DataFrame([result_dict])\n",
        "\n",
        "def estimate_eta_mu(data_by_person_time):\n",
        "    \"\"\"Estimate eta_mu using a formula of covariances which should in theory equal it.\n",
        "\n",
        "    \\frac{\\textrm{Cov}[\\textrm{predOut}_{i,t}, \\textrm{actualOut}_{i,t}] - \\frac{1}{2}\\textrm{Cov}[\\Delta\\textrm{predOut}_i, \\Delta\\textrm{actualOut}_i]}{\\textrm{Var}[\\textrm{actualOut}_{i,t}] - \\frac{1}{2}\\textrm{Var}[\\Delta\\textrm{actualOut}_i]}\n",
        "    \"\"\"\n",
        "    # TODO: there are cases where this fails, probably pathologies with outcomes being constant within person.\n",
        "    # TODO: wrap this with a function which performs bootstrap to get the standard errors.\n",
        "    covariances = data_by_person_time[['predicted_outcome', 'actual_outcome']].cov()\n",
        "    deltas = data_by_person_time.groupby('person_id')[['predicted_outcome', 'actual_outcome']].diff(1).dropna()\n",
        "    delta_covariances = deltas.cov()\n",
        "    eta_mu_hat = (covariances.iloc[0, 1] - 0.5 * delta_covariances.iloc[0, 1]) / (covariances.iloc[1, 1] - 0.5 * delta_covariances.iloc[1, 1])\n",
        "    return eta_mu_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Checks / tests"
      ],
      "metadata": {
        "id": "LGK6l2qA3AnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FE Regression is equivalent to diff-vs-diff regression, for two time periods.\n",
        "if False:\n",
        "    n_experiment_persons=13000; mean_mu=3200; std_dev_mu=1400; treatment_effect=200; person_time_error_sd=600\n",
        "    data_by_person_time = generate_data_by_person_time(n_experiment_persons=n_experiment_persons, mean_mu=mean_mu, std_dev_mu=std_dev_mu, treatment_effect=treatment_effect, person_time_error_sd=person_time_error_sd)\n",
        "    add_imperfect_prediction(data_by_person_time, eta_mu=0.7, eta_treatment=0.6, eta_epsilon=0.8, pred_error_nu_sd=400)\n",
        "    # Produce the same result for two time periods, up to decimal precision.\n",
        "    print(test_diff_prediction_diff_actual_regression(data_by_person_time))\n",
        "    print(test_within_person_prediction(data_by_person_time))"
      ],
      "metadata": {
        "id": "c9NSP2-ESBEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calibration"
      ],
      "metadata": {
        "id": "qEEBfxrQ3ERD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPkQn_C8ZhNR"
      },
      "outputs": [],
      "source": [
        "# Calibrate to R^2=0.92. So 600 comes out the best fit.\n",
        "std_dev_mu = 1400\n",
        "for person_time_error_sd in [0, 400, 600, 800, 1600]:\n",
        "    data_by_person_time = generate_data_by_person_time(n_experiment_persons=1000, mean_mu=3200, std_dev_mu=std_dev_mu,\n",
        "                                                       treatment_effect=0, person_time_error_sd=person_time_error_sd, n_time_periods=2)\n",
        "    print(person_time_error_sd, person_time_error_sd / std_dev_mu)\n",
        "    print(test_person_fixed_effects_explained_variance(data_by_person_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run simulations"
      ],
      "metadata": {
        "id": "r-BwAk9w3GQi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv-6hjCIZhNR"
      },
      "outputs": [],
      "source": [
        "n_experiment_persons=13000; mean_mu=3200; std_dev_mu=1400; treatment_effect=200; person_time_error_sd=600; n_time_periods=2\n",
        "data_by_person_time = generate_data_by_person_time(n_experiment_persons=n_experiment_persons, mean_mu=mean_mu, std_dev_mu=std_dev_mu, treatment_effect=treatment_effect, person_time_error_sd=person_time_error_sd, n_time_periods=n_time_periods)\n",
        "\n",
        "# Loop over many values of eta's, stds of components. Check how good the prediction is, and how good it is at finding treatment effect, and how well it does in the diffs prediction test, and the share of explained variance from person FE.\n",
        "# And SD of actual, pred\n",
        "result_df_list = []\n",
        "for eta_mu in tqdm.tqdm(np.linspace(0, 1, 5), position=0, leave=True):\n",
        "    for eta_treatment in np.linspace(0, 1, 5):\n",
        "        for eta_epsilon in np.linspace(0, 1, 5):\n",
        "            for pred_error_nu_sd in np.linspace(0, 1000, 5):\n",
        "                experiment_stats_df = pd.DataFrame({'n_experiment_persons': n_experiment_persons, 'mean_mu': mean_mu,\n",
        "                    'std_dev_mu': std_dev_mu, 'treatment_effect': treatment_effect, 'person_time_error_sd': person_time_error_sd,\n",
        "                    'eta_mu': eta_mu, 'eta_treatment': eta_treatment, 'eta_epsilon': eta_epsilon, 'pred_error_nu_sd': pred_error_nu_sd,\n",
        "                    'n_time_periods': n_time_periods}, index=[0])\n",
        "                add_imperfect_prediction(data_by_person_time, eta_mu, eta_treatment, eta_epsilon, pred_error_nu_sd)\n",
        "                test_stats_df = calc_prediction_stats(data_by_person_time)\n",
        "                eta_mu_hat = estimate_eta_mu(data_by_person_time)\n",
        "                test_stats_df['eta_mu_hat'] = eta_mu_hat\n",
        "                result_df_list.append(pd.concat([experiment_stats_df, test_stats_df], axis=1))\n",
        "\n",
        "result_df = pd.concat(result_df_list)\n",
        "result_df.fillna({'treatment_t': 0}, inplace=True)\n",
        "# Scale by actual treatment effect as found in the data, not the expected treatment effect.\n",
        "actual_treatment_effect = reg_summary(smf.ols('actual_outcome ~ is_treatment', data=data_by_person_time).fit()).query('variable == \"is_treatment[T.True]\"')['coef'].iloc[0]\n",
        "result_df['treatment_coef_relative'] = result_df['treatment_coef'] / actual_treatment_effect\n",
        "result_df['treatment_se_relative'] = result_df['treatment_se'] / actual_treatment_effect"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze simulation results"
      ],
      "metadata": {
        "id": "Hqz-0wFY3Nmb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CRS9UorZhNR"
      },
      "outputs": [],
      "source": [
        "labels = {'prediction_r2': 'ML Prediction R^2', 'treatment_coef_relative': 'Scaled Treatment Effect', 'treatment_t': 't-stat of Treatment Effect Coefficient', 'compression': 'Prediction Distribution Compression', 'diff_pred_coef': 'Diff-vs-diff Regression slope', 'probability': 'CDF'}\n",
        "result_df['eta_T'] = result_df['eta_treatment'].astype(str)\n",
        "# result_df.plot.scatter(x='eta_mu', y='treatment_t', log_y=True).show()\n",
        "# result_df.plot.scatter(x='eta_mu', y='treatment_coef_relative', trendline='ols').show()\n",
        "# result_df.plot.scatter(x='eta_mu', y='compression')\n",
        "\n",
        "# Using diff_pred_coef as the predictive measure\n",
        "result_df.plot.scatter(x='diff_pred_coef', y='treatment_coef_relative', color='eta_treatment', trendline='ols', width=600,\n",
        "                       title='without assuming eta_treatment = eta_epsilon,<br>correlation between diffs not informative')  # Almost no correspondence.\n",
        "result_df.query('eta_treatment == eta_epsilon').plot.scatter(x='diff_pred_coef', y='treatment_coef_relative', trendline='ols',\n",
        "                                                             color='eta_treatment', title='Correlation between differences is indicative of treatment compression',\n",
        "                                                             hover_data=['eta_treatment', 'eta_epsilon', 'pred_error_nu_sd', 'eta_mu'], width=600)  # Under this assumption, OK correspondence.\n",
        "result_df.query('eta_treatment == eta_epsilon').plot.scatter(x='diff_pred_coef', error_x='diff_pred_std_err', y='eta_epsilon', trendline='ols',\n",
        "                                                             color='eta_treatment', title='Actual and estimated eta_epsilon',\n",
        "                                                             hover_data=['eta_treatment', 'eta_epsilon', 'pred_error_nu_sd', 'eta_mu'], width=600)  # Good correspondence.\n",
        "\n",
        "# Distribution of coefficient with respect to theory (eta_epsilon) is the expected ~Normally distributed noise, with standard error as the standard deviation. But slightly overestimates.\n",
        "result_df['diff_pred_coef_z_score'] = result_df.eval('(diff_pred_coef - eta_epsilon) / diff_pred_std_err')\n",
        "px.ecdf(result_df, x='diff_pred_coef_z_score', color='eta_epsilon', labels=labels, width=600).add_hline(y=0.5, line_color='grey', line_width=2, line_dash='dash', opacity=0.5)\n",
        "px.ecdf(result_df, x='diff_pred_coef_z_score', color='pred_error_nu_sd', labels=labels, width=600).add_hline(y=0.5, line_color='grey', line_width=2, line_dash='dash', opacity=0.5)\n",
        "px.ecdf(result_df, x='diff_pred_coef_z_score', labels=labels, width=600).add_hline(y=0.5, line_color='grey', line_width=2, line_dash='dash', opacity=0.5)\n",
        "# Slight overestimation.\n",
        "result_df.groupby('pred_error_nu_sd')['diff_pred_coef_z_score'].agg(['mean', 'sem', 'std', 'median'])\n",
        "result_df.groupby('eta_epsilon')['diff_pred_coef_z_score'].agg(['mean', 'sem', 'std', 'median'])\n",
        "\n",
        "# Using diff_pred_r2 as the predictive measure\n",
        "result_df.plot.scatter(x='diff_pred_r2', y='treatment_coef_relative', color='eta_treatment', trendline='ols', width=600,\n",
        "                       title='without assuming eta_treatment = eta_epsilon,<br>correlation between diffs not informative')  # Almost no correspondence.\n",
        "result_df.query('eta_treatment == eta_epsilon').plot.scatter(x='diff_pred_r2', y='treatment_coef_relative', trendline='ols',\n",
        "                                                             color='eta_treatment', title='Correlation between differences is indicative of treatment compression',\n",
        "                                                             hover_data=['eta_treatment', 'eta_epsilon', 'pred_error_nu_sd', 'eta_mu'], width=600)  # Under this assumption, OK correspondence.\n",
        "result_df.query('eta_treatment == eta_epsilon').plot.scatter(x='diff_pred_r2', y='treatment_t', trendline='ols',\n",
        "                                                             title='Correlation between differences is indicative of statistical power',\n",
        "                                                             color='eta_treatment',\n",
        "                                                             hover_data=['eta_treatment', 'pred_error_nu_sd', 'eta_mu'], width=600)\n",
        "result_df.query('eta_treatment == eta_epsilon and eta_mu==0.5').plot.scatter(x='diff_pred_coef', y='treatment_coef_relative', trendline='ols',\n",
        "                                                                             hover_data=['eta_treatment', 'pred_error_nu_sd', 'eta_mu'])\n",
        "result_df.query('eta_mu == 0.5 and eta_treatment == eta_epsilon').plot.scatter(x='eta_treatment', y='treatment_coef_relative')\n",
        "# Treatment Effect regression slightly underestimates the expected treatment effect, even factoring in eta_treatment.\n",
        "add_y_equal_x_line(result_df.plot.scatter(x='eta_treatment', y='treatment_coef_relative', color='pred_error_nu_sd', labels=labels, width=600), 0, 1)\n",
        "result_df['treatment_coef_z_score'] = result_df.eval('(treatment_coef_relative - eta_treatment) / treatment_se_relative')\n",
        "result_df.groupby('eta_treatment')['treatment_coef_relative'].agg(['mean', 'std', 'median'])\n",
        "# Discard numerical instability S.E.~0 results.\n",
        "result_df.query('treatment_se_relative > 1E-10').groupby('eta_treatment')['treatment_coef_z_score'].agg(['mean', 'std', 'median'])\n",
        "result_df.sort_values('treatment_se_relative').iloc[:10]\n",
        "\n",
        "\n",
        "########################\n",
        "# Figures in the paper #\n",
        "########################\n",
        "# .query('eta_treatment == eta_epsilon')\n",
        "result_df.plot.scatter(x='prediction_r2', y='treatment_coef_relative', color='eta_T', trendline='ols', title='Better prediction does not guarantee more accurate treatment estimation', hover_data=['eta_T', 'pred_error_nu_sd', 'eta_mu'], width=600, labels=labels).show()\n",
        "result_df.query('eta_treatment == 0').sample(frac=1).plot.scatter(color='eta_mu', y='prediction_r2', x='eta_epsilon', trendline='ols', title='Better prediction is mostly affected by person attributes', hover_data=['eta_T', 'pred_error_nu_sd', 'eta_mu'], width=600, labels=labels).show()\n",
        "result_df.query('treatment_t < 1000').plot.scatter(x='prediction_r2', y='treatment_t', color='eta_treatment', trendline='ols', title='Better prediction does not guarantee more power', hover_data=['eta_T', 'pred_error_nu_sd', 'eta_mu'], width=600, labels=labels).show()\n",
        "\n",
        "result_df.plot.scatter(x='compression', y='treatment_coef_relative', color='eta_T', trendline='ols', width=600,\n",
        "                       title='Compression not very informative', labels=labels).show()  # These are only related if treatment is a big part of the variation and eta is large.\n",
        "\n",
        "# When eta_T = eta_epsilon, our estimation of eta_epsilon does a good job of predicting the scaled treatment effect (which tends to eta_T). Slightly over-predicts.\n",
        "fig = result_df.query('eta_treatment == eta_epsilon').plot.scatter(x='diff_pred_coef', y='treatment_coef_relative', # , error_x='diff_pred_std_err', trendline='ols'\n",
        "                                                             color='eta_T', title='Diff-vs-diff Regression slope predicts Scaled Treatment Effect<br>when eta_T = eta_epsilon',\n",
        "                                                             hover_data=['eta_T', 'eta_epsilon', 'pred_error_nu_sd', 'eta_mu'], labels=labels, width=600)\n",
        "add_y_equal_x_line(fig, 0, 1).show()\n",
        "\n",
        "# The same, without assuming eta_T = eta_epsilon. No predictive power.\n",
        "result_df.plot.scatter(x='diff_pred_coef', y='treatment_coef_relative', trendline='ols',\n",
        "                                                             color='eta_T', title='when eta_T != eta_epsilon, no predictive power',\n",
        "                                                             hover_data=['eta_T', 'eta_epsilon', 'pred_error_nu_sd', 'eta_mu'], labels=labels, width=600).show()\n",
        "\n",
        "\n",
        "# The prediction of differences cancels out the person FE, so what's left is eta_epsilon and exogenous prediction error:\n",
        "result_df.query('eta_treatment == eta_epsilon').plot.scatter(x='pred_error_nu_sd', y='diff_pred_r2', trendline='ols', color='eta_epsilon',\n",
        "                                                             title='Person FE cancel out, so eta_epsilon and nu is what is left', width=600,\n",
        "                                                             hover_data=['eta_T', 'pred_error_nu_sd', 'eta_mu'], labels=labels).show()  # Under this assumption, OK correspondence.\n",
        "\n",
        "\n",
        "# Estimating eta_mu\n",
        "fig = result_df.plot.scatter(x='eta_mu_hat', y='eta_mu') # , error_x='diff_pred_std_err', trendline='ols'\n",
        "                            # color='eta_T', title='Diff-vs-diff Regression slope predicts Scaled Treatment Effect<br>when eta_T = eta_epsilon',\n",
        "                            # hover_data=['eta_T', 'eta_epsilon', 'pred_error_nu_sd', 'eta_mu'], labels=labels, width=600)\n",
        "add_y_equal_x_line(fig, 0, 1).show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}